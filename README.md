# ðŸš² Bike Sharing Demand Prediction
This project trains a machine learning model to predict the total number of bike rentals per hour, based on time and weather data. 
It includes scripts to train the model, serve it via a Flask API, and test the API.

## Description
Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city. 
Using these systems, people are able rent a bike from a one location and return it to a different place on an as-needed basis. 
Currently, there are over 500 bike-sharing programs around the world.
The data generated by these systems makes them attractive for researchers because the duration of travel, departure location, arrival location, and time elapsed is explicitly recorded. 
Bike sharing systems therefore function as a sensor network, which can be used for studying mobility in a city.

## Setup & Installation (with uv)
These instructions assume you have uv installed. If not, see the official uv installation guide.

1. Clone the Repository:

```
git clone https://github.com/nicolas-hbt/mlzoomcamp-midterm-project.git
cd mlzoomcamp-midterm-project
```

2. Create and Activate a Virtual Environment:

```
# Create the virtual environment
uv venv

# Activate the environment
# On macOS/Linux:
source .venv/bin/activate
# On Windows (Cmd):
.\.venv\Scripts\activate
```

3. Install Dependencies: uv will use the requirements.txt file to install all necessary packages.

```
uv pip install -r requirements.txt
```

## Usage
There are three main ways to use this project:

1. Run the Prediction Server
This starts the Flask API server, which loads the pre-trained gb_model.pkl to serve predictions.

```
python predict.py
The server will be running at http://0.0.0.0:9696 (or http://localhost:9696).
```

2. Test the Prediction Server
While the server is running (in a separate terminal), you can run the test script to send a sample request.

```
python predict_test.py
You should see a JSON response with a prediction, like: {'predictions': [55.918...]}
```

3. (Optional) Re-train the Model
If you want to re-train the model from scratch:

```
python train.py
```

This will load train.csv, run the preprocessing and cross-validation, and save a new gb_model.pkl file.

## Data fields
datetime - hourly date + timestamp  
season -  1 = spring, 2 = summer, 3 = fall, 4 = winter 
holiday - whether the day is considered a holiday
workingday - whether the day is neither a weekend nor holiday
weather - 1: Clear, Few clouds, Partly cloudy, Partly cloudy
2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist
3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds
4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog 
temp - temperature in Celsius
atemp - "feels like" temperature in Celsius
humidity - relative humidity
windspeed - wind speed
casual - number of non-registered user rentals initiated
registered - number of registered user rentals initiated
count - number of total rentals

## Evaluation metrics
Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as

$$\sqrt{\frac{1}{n}\sum_{i=1}^n (log(p_i + 1) - log(a_i + 1))^2}$$
where:
+ $n$ is the number of hours in the test set 
+ $p_i$ is your predicted count
+ $a_i$ is the actual count
+ log(x) is the natural logarithm

## Important notes and misconceptions
Excerpt from the dataset guidelines: "the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. 
You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period."

For a real-world time-series problem, that would be an improper practice (known as "data leakage" or "look-ahead bias"). You should never train on data from the future to predict the past.

What this means is that we want the model to learn general hourly, daily (e.g., weekday vs. weekend), 
and weather-related patterns from the first 19 days. It's not learning a long-term "year-over-year" trend. It's learning an intra-month pattern.

